{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAHLR Week 10: Unsupervised Methods: Topic Modeling and Clustering\n",
    "\n",
    "Code notebook for TAHLR course at ISAW (Fall 2023) based on Albrecht et al. 2022 (Blueprints) Ch. 8: Unsupervised Methods: Topic Modeling and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get data from remote location\n",
    "\n",
    "# !mkdir -p ../data/blueprints\n",
    "# !curl -LJO https://github.com/blueprints-for-text-analytics-python/blueprints-text/raw/master/data/un-general-debates/un-general-debates-blueprint.csv.gz --output-dir ../data/blueprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "file = \"../data/blueprints/un-general-debates-blueprint.csv.gz\"\n",
    "debates = df = pd.read_csv(file)\n",
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data; spec. split into paragraphs\n",
    "\n",
    "import re\n",
    "df[\"paragraphs\"] = df[\"text\"].map(lambda text: re.split('[.?!]\\s*\\n', text))\n",
    "df[\"number_of_paragraphs\"] = df[\"paragraphs\"].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize paragraph count\n",
    "\n",
    "debates.groupby('year').agg({'number_of_paragraphs': 'mean'}).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make tf-idf matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "stopwords = [word for word in stopwords if word.isalpha()]\n",
    "\n",
    "tfidf_text = TfidfVectorizer(stop_words=stopwords, min_df=5, max_df=0.7)\n",
    "vectors_text = tfidf_text.fit_transform(debates['text'])\n",
    "vectors_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the paragraphs keeping the years\n",
    "\n",
    "paragraph_df = pd.DataFrame([{ \"text\": paragraph, \"year\": year } \n",
    "                               for paragraphs, year in \\\n",
    "                               zip(df[\"paragraphs\"], df[\"year\"]) \n",
    "                                    for paragraph in paragraphs if paragraph])\n",
    "\n",
    "tfidf_para_vectorizer = TfidfVectorizer(stop_words=stopwords, min_df=5,\n",
    "                                        max_df=0.7)\n",
    "tfidf_para_vectors = tfidf_para_vectorizer.fit_transform(paragraph_df[\"text\"])\n",
    "tfidf_para_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Creating a Topic Model for Paragraphs Using NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose, NMF\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf_text_model = NMF(n_components=10, random_state=42)\n",
    "W_text_matrix = nmf_text_model.fit_transform(tfidf_para_vectors)\n",
    "H_text_matrix = nmf_text_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for displaying topics\n",
    "\n",
    "def display_topics(model, features, no_top_words=5):\n",
    "    for topic, word_vector in enumerate(model.components_):\n",
    "        total = word_vector.sum()\n",
    "        largest = word_vector.argsort()[::-1] # invert sort order\n",
    "        print(\"\\nTopic %02d\" % topic)\n",
    "        for i in range(0, no_top_words):\n",
    "            print(\"  %s (%2.2f)\" % (features[largest[i]],\n",
    "                  word_vector[largest[i]]*100.0/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display NMF topics\n",
    "\n",
    "display_topics(nmf_text_model, tfidf_para_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Creating a Topic Model for Paragraphs with SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose, SVD\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd_para_model = TruncatedSVD(n_components = 10, random_state=42)\n",
    "W_svd_para_matrix = svd_para_model.fit_transform(tfidf_para_vectors)\n",
    "H_svd_para_matrix = svd_para_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display SVD topics\n",
    "\n",
    "display_topics(svd_para_model, tfidf_para_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Creating a Topic Model for Paragraphs with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce number of paragraphs (because LDA is computationally expensive)\n",
    "\n",
    "paragraph_df = paragraph_df[:50000]\n",
    "\n",
    "tfidf_para_vectorizer = TfidfVectorizer(stop_words=stopwords, min_df=5,\n",
    "                                        max_df=0.7)\n",
    "tfidf_para_vectors = tfidf_para_vectorizer.fit_transform(paragraph_df[\"text\"])\n",
    "tfidf_para_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Work with count vectors\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_para_vectorizer = CountVectorizer(stop_words=stopwords, min_df=5,\n",
    "                        max_df=0.7)\n",
    "count_para_vectors = count_para_vectorizer.fit_transform(paragraph_df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose, lda_para_model; nb: could take a long time\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda_para_model = LatentDirichletAllocation(n_components = 10, random_state=42)\n",
    "W_lda_para_matrix = lda_para_model.fit_transform(count_para_vectors)\n",
    "H_lda_para_matrix = lda_para_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lda_para_model, tfidf_para_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pyLDAvis to visualize topics\n",
    "\n",
    "# !pip install pyLDAvis\n",
    "import pyLDAvis.lda_model\n",
    "\n",
    "lda_display = pyLDAvis.lda_model.prepare(lda_para_model, count_para_vectors,\n",
    "                            count_para_vectorizer, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize topic \"weights\" with word cloud\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "def wordcloud_topics(model, features, no_top_words=40):\n",
    "    for topic, words in enumerate(model.components_):\n",
    "        size = {}\n",
    "        largest = words.argsort()[::-1] # invert sort order\n",
    "        for i in range(0, no_top_words):\n",
    "            size[features[largest[i]]] = abs(words[largest[i]])\n",
    "        wc = WordCloud(background_color=\"white\", max_words=100,\n",
    "                       width=960, height=540)\n",
    "        wc.generate_from_frequencies(size)\n",
    "        plt.figure(figsize=(12,12))\n",
    "        plt.imshow(wc, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        # if you don't want to save the topic model, comment the next line\n",
    "        plt.savefig(f'topic{topic}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_topics(lda_para_model, count_para_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Blueprint: Kmeans clustering w. visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://medium.com/mlearning-ai/text-clustering-with-tf-idf-in-python-c94cd26a31e7\n",
    "\n",
    "# Set up kmeans\n",
    "\n",
    "X = tfidf_para_vectors\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X)\n",
    "clusters = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensions with PCA\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "pca_vecs = pca.fit_transform(X.toarray())\n",
    "x0 = pca_vecs[:, 0]\n",
    "x1 = pca_vecs[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update dataframe\n",
    "\n",
    "paragraph_df[\"cluster\"] = clusters\n",
    "paragraph_df[\"x0\"] = x0\n",
    "paragraph_df[\"x1\"] = x1\n",
    "paragraph_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function, get top keywords\n",
    "\n",
    "def get_top_keywords(n_terms):\n",
    "    \"\"\"This function returns the keywords for each centroid of the KMeans\"\"\"\n",
    "    df = pd.DataFrame(X.todense()).groupby(clusters).mean() # groups the TF-IDF vector by cluster\n",
    "    terms = tfidf_para_vectorizer.get_feature_names_out() # access tf-idf terms\n",
    "    for i,r in df.iterrows():\n",
    "        print('\\nCluster {}'.format(i))\n",
    "        print(','.join([terms[t] for t in np.argsort(r)[-n_terms:]])) # for each row of the dataframe, find the n terms that have the highest tf idf score\n",
    "            \n",
    "get_top_keywords(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map clusters to appropriate labels \n",
    "\n",
    "cluster_map = {0: \"africa\", 1: \"general\", 2: \"china\"}\n",
    "paragraph_df['cluster'] = paragraph_df['cluster'].map(cluster_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters with Seaborn\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "plt.title(\"TF-IDF + KMeans on UN General Debates\", fontdict={\"fontsize\": 18})\n",
    "\n",
    "plt.xlabel(\"X0\", fontdict={\"fontsize\": 16})\n",
    "plt.ylabel(\"X1\", fontdict={\"fontsize\": 16})\n",
    "\n",
    "sns.scatterplot(data=paragraph_df, x='x0', y='x1', hue='cluster', palette=\"viridis\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tahlr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
