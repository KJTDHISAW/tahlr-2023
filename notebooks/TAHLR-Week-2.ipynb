{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Book: Ch. 2. Accessing Text Corpora and Lexical Resources\n",
    "\n",
    "Code-only version of Chapter 21 of the [NLTK Book](https://www.nltk.org/book/ch02.html) for use in the TAHLR course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Accessing Text Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Gutenberg corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from pprint import pprint\n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
    "len(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma_Text = nltk.Text(emma)\n",
    "emma_Text.concordance(\"surprize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homer_text = \"\"\"μῆνιν ἄειδε θεὰ Πηληϊάδεω Ἀχιλῆος\n",
    "οὐλομένην, ἣ μυρί᾽ Ἀχαιοῖς ἄλγε᾽ ἔθηκε,\n",
    "πολλὰς δ᾽ ἰφθίμους ψυχὰς Ἄϊδι προΐαψεν\n",
    "ἡρώων, αὐτοὺς δὲ ἑλώρια τεῦχε κύνεσσιν\n",
    "οἰωνοῖσί τε πᾶσι, Διὸς δ᾽ ἐτελείετο βουλή,\n",
    "ἐξ οὗ δὴ τὰ πρῶτα διαστήτην ἐρίσαντε\n",
    "Ἀτρεΐδης τε ἄναξ ἀνδρῶν καὶ δῖος Ἀχιλλεύς.\n",
    "τίς τ᾽ ἄρ σφωε θεῶν ἔριδι ξυνέηκε μάχεσθαι;\n",
    "Λητοῦς καὶ Διὸς υἱός: ὃ γὰρ βασιλῆϊ χολωθεὶς\n",
    "νοῦσον ἀνὰ στρατὸν ὄρσε κακήν, ὀλέκοντο δὲ λαοί,\n",
    "οὕνεκα τὸν Χρύσην ἠτίμασεν ἀρητῆρα\n",
    "Ἀτρεΐδης: ὃ γὰρ ἦλθε θοὰς ἐπὶ νῆας Ἀχαιῶν\n",
    "λυσόμενός τε θύγατρα φέρων τ᾽ ἀπερείσι᾽ ἄποινα,\n",
    "στέμματ᾽ ἔχων ἐν χερσὶν ἑκηβόλου Ἀπόλλωνος\n",
    "χρυσέῳ ἀνὰ σκήπτρῳ, καὶ λίσσετο πάντας Ἀχαιούς,\n",
    "Ἀτρεΐδα δὲ μάλιστα δύω, κοσμήτορε λαῶν:\n",
    "Ἀτρεΐδαι τε καὶ ἄλλοι ἐϋκνήμιδες Ἀχαιοί,\n",
    "ὑμῖν μὲν θεοὶ δοῖεν Ὀλύμπια δώματ᾽ ἔχοντες\n",
    "ἐκπέρσαι Πριάμοιο πόλιν, εὖ δ᾽ οἴκαδ᾽ ἱκέσθαι:\n",
    "παῖδα δ᾽ ἐμοὶ λύσαιτε φίλην, τὰ δ᾽ ἄποινα δέχεσθαι,\n",
    "ἁζόμενοι Διὸς υἱὸν ἑκηβόλον Ἀπόλλωνα.\n",
    "\n",
    "ἔνθ᾽ ἄλλοι μὲν πάντες ἐπευφήμησαν Ἀχαιοὶ\n",
    "αἰδεῖσθαί θ᾽ ἱερῆα καὶ ἀγλαὰ δέχθαι ἄποινα:\n",
    "ἀλλ᾽ οὐκ Ἀτρεΐδῃ Ἀγαμέμνονι ἥνδανε θυμῷ,\n",
    "ἀλλὰ κακῶς ἀφίει, κρατερὸν δ᾽ ἐπὶ μῦθον ἔτελλε:\n",
    "μή σε γέρον κοίλῃσιν ἐγὼ παρὰ νηυσὶ κιχείω\n",
    "ἢ νῦν δηθύνοντ᾽ ἢ ὕστερον αὖτις ἰόντα,\n",
    "μή νύ τοι οὐ χραίσμῃ σκῆπτρον καὶ στέμμα θεοῖο:\n",
    "τὴν δ᾽ ἐγὼ οὐ λύσω: πρίν μιν καὶ γῆρας ἔπεισιν\n",
    "ἡμετέρῳ ἐνὶ οἴκῳ ἐν Ἄργεϊ τηλόθι πάτρης\n",
    "ἱστὸν ἐποιχομένην καὶ ἐμὸν λέχος ἀντιόωσαν:\n",
    "ἀλλ᾽ ἴθι μή μ᾽ ἐρέθιζε σαώτερος ὥς κε νέηαι.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homer_tokens = homer_text.split()\n",
    "print(homer_tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homer_Text = nltk.Text(homer_tokens)\n",
    "print(homer_Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(homer_Text)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(homer_Text.concordance(\"καὶ\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown.words(categories='news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown.words(fileids=['cg22'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = brown.sents(categories=['news', 'editorial', 'reviews'])\n",
    "\n",
    "for sent in sents[:5]:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "news_text = brown.words(categories='news')\n",
    "fdist = nltk.FreqDist(w.lower() for w in news_text)\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "for m in modals:\n",
    "    print(m + ':', fdist[m], end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "           (genre, word)\n",
    "           for genre in brown.categories()\n",
    "           for word in brown.words(categories=genre))\n",
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "cfd.tabulate(conditions=genres, samples=modals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5   Inaugural Address Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural\n",
    "pprint(inaugural.fileids()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[fileid[:4] for fileid in inaugural.fileids()[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "           (target, fileid[:4])\n",
    "           for fileid in inaugural.fileids()\n",
    "           for w in inaugural.words(fileid)\n",
    "           for target in ['america', 'citizen']\n",
    "           if w.lower().startswith(target))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cfd.plot(linewidth=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6   Annotated Text Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to the [NLTK Book](https://www.nltk.org/book/ch02.html) for more information on the above corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Corpora in other languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.cess_esp.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('indian')\n",
    "nltk.corpus.indian.words('hindi.pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Universal Declaration of Human Rights corpus\n",
    "nltk.corpus.udhr.fileids()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.udhr.words('Javanese-Latin1')[11:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.udhr.encoding('Javanese-Latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.udhr.encoding('Abkhaz-Cyrillic+Abkh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.udhr.encoding('Abkhaz-UTF8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import udhr\n",
    "languages = ['Chickasaw', 'English', 'German_Deutsch',\n",
    "     'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "           (lang, len(word))\n",
    "           for lang in languages\n",
    "           for word in udhr.words(lang + '-Latin1'))\n",
    "figure(figsize=(15, 5))\n",
    "cfd.plot(cumulative=True, percents=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Text corpus structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg = nltk.corpus.gutenberg\n",
    "raw = gutenberg.raw(\"burgess-busterbrown.txt\")\n",
    "raw[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = gutenberg.words(\"burgess-busterbrown.txt\")\n",
    "print(words[1:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = gutenberg.sents(\"burgess-busterbrown.txt\")\n",
    "for sent in sents[:5]:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 Loading your own corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = '../data/texts/lyoc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = PlaintextCorpusReader(corpus_root, '.*')\n",
    "print(H.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(H.words('homer-odyssey-1.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(H.words('homer-odyssey-2.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Conditional frequency distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Conditions and events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said']\n",
    "genre = ['news'] * len(text)\n",
    "print(text)\n",
    "print(genre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = list(zip(genre, text))\n",
    "print(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Counting words by genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "          (genre, word)\n",
    "           for genre in brown.categories()\n",
    "           for word in brown.words(categories=genre))\n",
    "print(cfd.conditions())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_word = [(genre, word)\n",
    "              for genre in ['news', 'romance'] \n",
    "              for word in brown.words(categories=genre)]\n",
    "len(genre_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(genre_word[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(genre_word[-4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(genre_word)\n",
    "cfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd.conditions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cfd['news'])\n",
    "print(cfd['romance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd['romance'].most_common(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd['romance']['could']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Plotting and tabulating distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "          (target, fileid[:4])\n",
    "          for fileid in inaugural.fileids()\n",
    "          for w in inaugural.words(fileid)\n",
    "          for target in ['america', 'citizen']\n",
    "          if w.lower().startswith(target))\n",
    "\n",
    "figure(figsize=(15, 5))\n",
    "cfd.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = ['Chickasaw', 'English', 'German_Deutsch',\n",
    "    'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "          (lang, len(word))\n",
    "          for lang in languages\n",
    "          for word in udhr.words(lang + '-Latin1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd.tabulate(conditions=['English', 'German_Deutsch'], samples=range(10), cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd.tabulate(conditions=['English', 'German_Deutsch'], samples=range(10), cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd.tabulate(conditions=['English', 'German_Deutsch'], samples=range(15,26), cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Generating random text with bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. More Python\n",
    "\n",
    "Refer to the [NLTK Book](https://www.nltk.org/book/ch02.html) for more information on the following topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lexical resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Wordlist Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.corpus.words.words()[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unusual_words(text):\n",
    "    text_vocab = set(w.lower() for w in text if w.isalpha())\n",
    "    english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "    unusual = text_vocab - english_vocab\n",
    "    return sorted(unusual)\n",
    "\n",
    "print(unusual_words(nltk.corpus.gutenberg.words('whitman-leaves.txt'))[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puzzle_letters = nltk.FreqDist('cgilorw')\n",
    "obligatory = 'o'\n",
    "wordlist = nltk.corpus.words.words()\n",
    "\n",
    "\n",
    "answers = [w for w in wordlist if len(w) >= 4 \n",
    "                     and obligatory in w \n",
    "                      and nltk.FreqDist(set(w)) <= puzzle_letters]\n",
    "\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = nltk.corpus.names\n",
    "names.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_names = names.words('male.txt')\n",
    "female_names = names.words('female.txt')\n",
    "[w for w in male_names if w in female_names][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "           (fileid, name[-1])\n",
    "          for fileid in names.fileids()\n",
    "           for name in names.words(fileid))\n",
    "figure(figsize=(15, 5))\n",
    "cfd.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5  WordNet\n",
    "\n",
    "Refer to the [NLTK Book](https://www.nltk.org/book/ch02.html) for more information on the above corpora."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tahlr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
