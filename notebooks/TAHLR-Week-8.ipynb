{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAHLR Week 8: Feature Engineering and Syntactic Similarity\n",
    "\n",
    "Code notebook for TAHLR course at ISAW (Fall 2023) based on Albrecht et al. 2022 (Blueprints) Ch. 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "# spacy.cli.download('en_core_web_sm') # Download if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of sentences\n",
    "\n",
    "sentences = [\"It was the best of times\",\n",
    "             \"it was the worst of times\",\n",
    "             \"it was the age of wisdom\",\n",
    "             \"it was the age of foolishness\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences\n",
    "\n",
    "tokenized_sentences = [[t for t in sentence.split()] for sentence in sentences]\n",
    "pprint(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vocabulary\n",
    "vocabulary = list(set([w for s in tokenized_sentences for w in s]))\n",
    "pprint(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show datafram \n",
    "\n",
    "pd.DataFrame([[w, i] for i,w in enumerate(vocabulary)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize documents\n",
    "\n",
    "def onehot_encode(tokenized_sentence):\n",
    "    return [1 if w in tokenized_sentence else 0 for w in vocabulary]\n",
    "\n",
    "onehot = [onehot_encode(tokenized_sentence)\n",
    "         for tokenized_sentence in tokenized_sentences]\n",
    "\n",
    "for (sentence, oh) in zip(sentences, onehot):\n",
    "    print(\"%s: %s\" % (oh, sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with OOV\n",
    "\n",
    "onehot_encode(\"John likes to watch movies. Mary likes movies too.\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show OHE matrix\n",
    "\n",
    "pd.DataFrame(onehot, columns=vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating \"bitwise\" similarity\n",
    "\n",
    "sim = [onehot[0][i] & onehot[1][i] for i in range(0, len(vocabulary))]\n",
    "sum(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating similarity, dot product\n",
    "\n",
    "np.dot(onehot[0], onehot[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating similarity matrix\n",
    "\n",
    "np.dot(onehot, np.transpose(onehot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding with scikit-learn\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit_transform(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Using scikit-learn's CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_sentences = sentences + \\\n",
    "                 [\"John likes to watch movies. Mary likes movies too.\",\n",
    "                  \"Mary also likes to watch football games.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit sentences\n",
    "\n",
    "cv.fit(more_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show params\n",
    "\n",
    "pprint(cv.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show \"vocabulary\"\n",
    "\n",
    "print(cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform sentences\n",
    "\n",
    "dt = cv.transform(more_sentences)\n",
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make DTM\n",
    "\n",
    "pd.DataFrame(dt.toarray(), columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Calculating similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(more_sentences[0])\n",
    "print(more_sentences[1])\n",
    "\n",
    "cosine_similarity(dt[0], dt[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity matrix\n",
    "\n",
    "pd.DataFrame(cosine_similarity(dt, dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF models\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_dt = tfidf.fit_transform(dt)\n",
    "pd.DataFrame(tfidf_dt.toarray(), columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show similarity matrix\n",
    "\n",
    "pd.DataFrame(cosine_similarity(tfidf_dt, tfidf_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Reducting feature dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from remote location\n",
    "\n",
    "!mkdir -p ../data/blueprints\n",
    "!curl -LJO https://github.com/blueprints-for-text-analytics-python/blueprints-text/raw/master/data/abcnews/abcnews-date-text.csv.gz --output-dir ../data/blueprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "\n",
    "headlines = pd.read_csv(\"../data/blueprints/abcnews-date-text.csv.gz\", parse_dates=[\"publish_date\"])\n",
    "print(len(headlines))\n",
    "headlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF vectorization\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "dt = tfidf.fit_transform(headlines[\"headline_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Show\" matrix\n",
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of rows was expected, but the number of columns (the vocabulary) is really large, with almost 100,000 words. Doing the math shows that a naive storage of data would have led to  1,103,663 * 95,878 elements with 8 bytes per float and have used roughly 788 GB RAM. This shows the incredible effectiveness of sparse matrices as the real memory used is “only” 56,010,856 bytes (roughly 0.056 GB; found out via dt.data.nbytes). It’s still a lot, but it’s manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "stopwords = [w for w in stopwords if w.isalpha()]\n",
    "print(len(stopwords))\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=list(stopwords))\n",
    "dt = tfidf.fit_transform(headlines[\"headline_text\"])\n",
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum frequency\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=stopwords, min_df=2)\n",
    "dt = tfidf.fit_transform(headlines[\"headline_text\"])\n",
    "dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Improving Features by Making Them More Specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing linguistic analysis with spaCy\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nouns_adjectives_verbs = [\"NOUN\", \"PROPN\", \"ADJ\", \"ADV\", \"VERB\"]\n",
    "\n",
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "def lemmatize_nouns_adjectives_verbs(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc\n",
    "                     if token.pos_ in nouns_adjectives_verbs])\n",
    "\n",
    "ROWS = 100000\n",
    "\n",
    "headlines = headlines.sample(ROWS, random_state=42)\n",
    "\n",
    "headlines['lemmas'] = headlines['headline_text'].progress_apply(lemmatize)\n",
    "headlines['nav'] = headlines['headline_text'].progress_apply(lemmatize_nouns_adjectives_verbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Using Lemmas Instead of Words for Vectorizing Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF vectorization after lemmatization\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=stopwords)\n",
    "dt = tfidf.fit_transform(headlines[\"lemmas\"].map(str))\n",
    "dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Adding context via n-grams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF vectorization with ngams\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2),\n",
    "        stop_words=stopwords)\n",
    "dt = tfidf.fit_transform(headlines[\"nav\"].map(str))\n",
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show matrix\n",
    "\n",
    "pd.DataFrame(dt.toarray(), columns=tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using custom stopwords\n",
    "\n",
    "# NB: There are \"test\" headlines in the corpus\n",
    "stopwords += ['test']\n",
    "tfidf = TfidfVectorizer(stop_words=stopwords, ngram_range=(1,2), min_df=2, \\\n",
    "                        norm='l2')\n",
    "dt = tfidf.fit_transform(headlines[\"headline_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Finding Most Similar Headlines to a Made-up Headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make up new headline; transform\n",
    "\n",
    "made_up = tfidf.transform([\"what is happening in sydney\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get similarity of this made-up headline to all other headlines\n",
    "\n",
    "sim = cosine_similarity(made_up, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the row with the most similar headline, i.e. the one with the highest similarity\n",
    "\n",
    "np.argmax(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show that row\n",
    "\n",
    "headlines.iloc[np.argmax(sim)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example\n",
    "\n",
    "made_up = tfidf.transform([\"what is happening at the sydney opera house\"])\n",
    "sim = cosine_similarity(made_up, dt)\n",
    "headlines.iloc[np.argmax(sim)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Finding the Two Most Similar Documents in a Large Corpus (Much More Difficult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might think that finding the most similar documents in the corpus is as easy as calculating the cosine_similarity between all documents. However, this is not possible as 1,103,663 × 1,103,663 = 1,218,072,017,569. More than one trillion elements do not fit in the RAM of even the most advanced computers. It is perfectly possible to perform the necessary matrix multiplications without having to wait for ages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we are starting with\n",
    "\n",
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A process to batch and progressively calculate the similarity matrix\n",
    "\n",
    "%%time\n",
    "batch = 10000\n",
    "max_sim = 0.0\n",
    "max_a = None\n",
    "max_b = None\n",
    "for a in range(0, dt.shape[0], batch):\n",
    "    for b in range(0, a+batch, batch):\n",
    "        print(a, b)\n",
    "        r = np.dot(dt[a:a+batch], np.transpose(dt[b:b+batch]))\n",
    "        # eliminate identical vectors\n",
    "        # by setting their similarity to 0 which gets sorted out\n",
    "        r[r > 0.9999] = 0\n",
    "        sim = r.max()\n",
    "        if sim > max_sim:\n",
    "            # argmax returns a single value which we have to\n",
    "            # map to the two dimensions\n",
    "            (max_a, max_b) = np.unravel_index(np.argmax(r), r.shape)\n",
    "            # adjust offsets in corpus (this is a submatrix)\n",
    "            max_a += a\n",
    "            max_b += b\n",
    "            max_sim = sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the batched similarity best result\n",
    "\n",
    "print(headlines.iloc[max_a])\n",
    "print(headlines.iloc[max_b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Finding Related Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF for limited vocabulary\n",
    "\n",
    "tfidf_word = TfidfVectorizer(stop_words=stopwords, min_df=100)\n",
    "dt_word = tfidf_word.fit_transform(headlines[\"headline_text\"])\n",
    "dt_word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get similarity matrix\n",
    "\n",
    "# NB: With ~1000 results we can do this in memory\n",
    "r = cosine_similarity(dt_word.T, dt_word.T)\n",
    "np.fill_diagonal(r, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep data\n",
    "\n",
    "voc = tfidf_word.get_feature_names_out()\n",
    "size = r.shape[0] # quadratic\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore matrix\n",
    "\n",
    "print(r)\n",
    "print()\n",
    "print(r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore matrix, 2\n",
    "\n",
    "print(r[0])\n",
    "print(r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore flattened matrix\n",
    "\n",
    "print(list(r.flatten()[:3]))\n",
    "print(list(r.flatten()[1030:1030+3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 40 results by position\n",
    "\n",
    "argsorts = np.argsort(r.flatten())[::-1][:40]\n",
    "argsorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at dividing flattened matrix by original size\n",
    "\n",
    "print(argsorts[0])\n",
    "print(argsorts[0] / size)\n",
    "voc[int(argsorts[0] / size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at dividing flattened matrix by original size, 2\n",
    "\n",
    "print(int(argsorts[0] % size))\n",
    "voc[int(argsorts[0] % size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through max args and print related words\n",
    "\n",
    "for index in np.argsort(r.flatten())[::-1][0:40]:\n",
    "    a = int(index/size)\n",
    "    b = index%size\n",
    "    if a > b:  # avoid repetitions\n",
    "        # print(index)\n",
    "        # print(size)\n",
    "        # print(a, b)\n",
    "        print('\"%s\" related to \"%s\"' % (voc[a], voc[b]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tahlr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
