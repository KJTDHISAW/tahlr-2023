{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAHLR Supplement: Conditional Frequency Distribution tutorial\n",
    "\n",
    "This TAHLR supplement shows how to build a conditional frequency distribution on your own set of files, usign the first three books of Homer's *Odyssey* (as found in the `data/texts/lyoc` folder from Week 2). The specific example shows which is more frequent—καί or καὶ, i.e. the word for 'and' with either an acute or grave accent—in each of the first three books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Get list of files\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "files = glob('../data/texts/lyoc/*.txt')\n",
    "\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Make list of texts\n",
    "\n",
    "texts = []\n",
    "\n",
    "for file in files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        texts.append(text)\n",
    "\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Make list of tokenized texts (using list comprehension)\n",
    "\n",
    "tokenized_texts = [word_tokenize(text) for text in texts]\n",
    "print(len(tokenized_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make list of tokenized texts; nb: same as above without list comprehension\n",
    "\n",
    "# tokenized_texts = []\n",
    "\n",
    "# for text in texts:\n",
    "#     tokenized_texts.append(word_tokenize(text))\n",
    "\n",
    "# print(len(tokenized_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4102\n",
      "4102\n"
     ]
    }
   ],
   "source": [
    "# Make a list of labels; i.e. these will be the 'conditions' in the cfd\n",
    "\n",
    "label_1 = 'odyssey-1'\n",
    "label_2 = 'odyssey-2'\n",
    "label_3 = 'odyssey-3'\n",
    "\n",
    "\n",
    "# Note how the list of labels is the same length as the list of tokenized texts\n",
    "labels_1 = [label_1] * len(tokenized_texts[0])\n",
    "labels_2 = [label_2] * len(tokenized_texts[1])\n",
    "labels_3 = [label_3] * len(tokenized_texts[2])\n",
    "\n",
    "print(len(tokenized_texts[0]))\n",
    "print(len(labels_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('odyssey-1', 'ἦμος'), ('odyssey-1', 'δ'), ('odyssey-1', \"'\"), ('odyssey-1', 'ἠριγένεια'), ('odyssey-1', 'φάνη'), ('odyssey-1', 'ῥοδοδάκτυλος'), ('odyssey-1', '̓Ηώς'), ('odyssey-1', ','), ('odyssey-1', 'ὤρνυτ'), ('odyssey-1', \"'\")]\n"
     ]
    }
   ],
   "source": [
    "# Create lists of (label, token) tuples\n",
    "\n",
    "labelled_tokens_1 = list(zip(labels_1, tokenized_texts[0]))\n",
    "labelled_tokens_2 = list(zip(labels_2, tokenized_texts[1]))\n",
    "labelled_tokens_3 = list(zip(labels_3, tokenized_texts[2]))\n",
    "\n",
    "print(labelled_tokens_1[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one long list of all the (label, token) tuples\n",
    "\n",
    "labelled_tokens = labelled_tokens_1 + labelled_tokens_2 + labelled_tokens_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a conditional frequency distribution from the labelled tokens\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(labelled_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'\", 350),\n",
       " (',', 299),\n",
       " ('δ', 113),\n",
       " ('.', 111),\n",
       " (':', 106),\n",
       " ('καὶ', 83),\n",
       " ('δὲ', 36),\n",
       " ('τε', 31),\n",
       " ('οἱ', 28),\n",
       " ('δέ', 25)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the most common tokens for the first label\n",
    "\n",
    "cfd['odyssey-1'].most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          καί καὶ \n",
      "odyssey-1   5  83 \n",
      "odyssey-2   2  88 \n",
      "odyssey-3   5  69 \n"
     ]
    }
   ],
   "source": [
    "# Tabulate the frequency of specific tokens (i.e. samples) for each label (i.e. conditions)\n",
    "\n",
    "cfd.tabulate(conditions=[label_1, label_2, label_3], samples=['καί', 'καὶ'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tahlr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
