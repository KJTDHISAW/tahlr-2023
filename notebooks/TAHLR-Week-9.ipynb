{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAHLR Week 9: Text Classification Algorithms\n",
    "\n",
    "Code notebook for TAHLR course at ISAW (Fall 2023) based on Albrecht et al. 2022 (Blueprints) Ch. 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "from blueprints import clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the Java Development Tools Bug Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/blueprints_6/eclipse_jdt.csv')\n",
    "print (df.columns)\n",
    "df[['Issue_id','Priority','Component','Title','Description']].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Priority'].value_counts().sort_index().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Component'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blueprint: Building a Text Classification System\n",
    "\n",
    "#### Step 1: Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Title','Description','Priority']]\n",
    "df = df.dropna()\n",
    "df['text'] = df['Title'] + ' ' + df['Description']\n",
    "df = df.drop(columns=['Title','Description'])\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(clean)\n",
    "df = df[df['text'].str.len() > 50]\n",
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(df['text'],\n",
    "                                                    df['Priority'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=df['Priority'])\n",
    "\n",
    "print('Size of Training Data ', X_train.shape[0])\n",
    "print('Size of Test Data ', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Training the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df = 10, ngram_range=(1,2), stop_words=\"english\")\n",
    "X_train_tf = tfidf.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "# NB: \"The SVM algorithm is preferred when working with text data because it is more suited to work with sparse data compared to other algorithms like Random Forest.\"\n",
    "\n",
    "model1 = LinearSVC(random_state=0, tol=1e-5, dual=True)\n",
    "model1.fit(X_train_tf, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show parameters\n",
    "\n",
    "model1.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tf = tfidf.transform(X_test)\n",
    "\n",
    "Y_pred = model1.predict(X_test_tf)\n",
    "print ('Accuracy Score - ', accuracy_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DummyClassifier(strategy='most_frequent')\n",
    "clf.fit(X_train, Y_train)\n",
    "Y_pred_baseline = clf.predict(X_test)\n",
    "print ('Accuracy Score - ', accuracy_score(Y_test, Y_pred_baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model1.predict(X_test_tf)\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb: plot_confusion_matrix as shown in *Blueprints* is deprecated; use ConfusionMatrixDisplay instead as shown below [PJB 11.3.2023]\n",
    "\n",
    "CMD = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model1.classes_)\n",
    "CMD.plot(cmap='Blues');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test, Y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter bug reports with priority P3 and sample 4000 rows from it\n",
    "df_sampleP3 = df[df['Priority'] == 'P3'].sample(n=4000)\n",
    "\n",
    "# Create a separate DataFrame containing all other bug reports\n",
    "df_sampleRest = df[df['Priority'] != 'P3']\n",
    "\n",
    "# Concatenate the two DataFrame to create the new balanced bug reports dataset\n",
    "df_balanced = pd.concat([df_sampleRest, df_sampleP3])\n",
    "\n",
    "# Check the status of the class imbalance\n",
    "df_balanced['Priority'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Blueprint for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the balanced DataFrame\n",
    "\n",
    "df = df_balanced[['text', 'Priority']]\n",
    "df = df.dropna()\n",
    "\n",
    "# Step 1 - Data Preparation\n",
    "\n",
    "df['text'] = df['text'].apply(clean)\n",
    "\n",
    "# Step 2 - Train-Test Split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df['text'],\n",
    "                                                    df['Priority'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=df['Priority'])\n",
    "print('Size of Training Data ', X_train.shape[0])\n",
    "print('Size of Test Data ', X_test.shape[0])\n",
    "\n",
    "# Step 3 - Training the Machine Learning model\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=10, ngram_range=(1, 2), stop_words=\"english\")\n",
    "X_train_tf = tfidf.fit_transform(X_train)\n",
    "\n",
    "model1 = LinearSVC(random_state=0, tol=1e-5, dual=True)\n",
    "model1.fit(X_train_tf, Y_train)\n",
    "\n",
    "# Step 4 - Model Evaluation\n",
    "\n",
    "X_test_tf = tfidf.transform(X_test)\n",
    "Y_pred = model1.predict(X_test_tf)\n",
    "print('Accuracy Score - ', accuracy_score(Y_test, Y_pred))\n",
    "print()\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DummyClassifier(strategy='stratified')\n",
    "clf.fit(X_train, Y_train)\n",
    "Y_pred_baseline = clf.predict(X_test)\n",
    "print ('Accuracy Score - ', accuracy_score(Y_test, Y_pred_baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame combining the Title and Description,\n",
    "# Actual and Predicted values that we can explore\n",
    "\n",
    "frame = { 'text': X_test, 'actual': Y_test, 'predicted': Y_pred }\n",
    "result = pd.DataFrame(frame)\n",
    "\n",
    "result[((result['actual'] == 'P1') | (result['actual'] == 'P2')) &\n",
    "       (result['actual'] == result['predicted'])].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[((result['actual'] == 'P1') | (result['actual'] == 'P2')) &\n",
    "       (result['actual'] != result['predicted'])].sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Using Cross-Validation to Estimate Realistic Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df = 10, ngram_range=(1,2), stop_words=\"english\")\n",
    "df_tf = tfidf.fit_transform(df['text']).toarray()\n",
    "\n",
    "# Cross Validation with 5 folds\n",
    "\n",
    "scores = cross_val_score(estimator=model1,\n",
    "                         X=df_tf,\n",
    "                         y=df['Priority'],\n",
    "                         cv=5)\n",
    "\n",
    "print (\"Validation scores from each iteration of the cross validation \", scores)\n",
    "print (\"Mean value across of validation scores \", scores.mean())\n",
    "print (\"Standard deviation of validation scores \", scores.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Using Cross-Validation to Estimate Realistic Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline = Pipeline(\n",
    "    steps=[('tfidf', TfidfVectorizer(stop_words=\"english\")),\n",
    "            ('model', LinearSVC(random_state=42, tol=1e-5, dual=True))])\n",
    "\n",
    "grid_param = [{\n",
    "    'tfidf__min_df': [5, 10],\n",
    "    'tfidf__ngram_range': [(1, 3), (1, 6)],\n",
    "    'model__penalty': ['l2'],\n",
    "    'model__loss': ['hinge'],\n",
    "    'model__max_iter': [10000]\n",
    "}, {\n",
    "    'tfidf__min_df': [5, 10],\n",
    "    'tfidf__ngram_range': [(1, 3), (1, 6)],\n",
    "    'model__C': [1, 10],\n",
    "    'model__tol': [1e-2, 1e-3]\n",
    "}]\n",
    "\n",
    "gridSearchProcessor = GridSearchCV(estimator=training_pipeline,\n",
    "                                   param_grid=grid_param,\n",
    "                                   cv=5)\n",
    "gridSearchProcessor.fit(df['text'], df['Priority'])\n",
    "\n",
    "best_params = gridSearchProcessor.best_params_\n",
    "print(\"Best alpha parameter identified by grid search \", best_params)\n",
    "\n",
    "best_result = gridSearchProcessor.best_score_\n",
    "print(\"Best result identified by grid search \", best_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_results = pd.DataFrame(gridSearchProcessor.cv_results_)\n",
    "gridsearch_results[['rank_test_score', 'mean_test_score',\n",
    "                    'params']].sort_values(by=['rank_test_score'])[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tahlr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
