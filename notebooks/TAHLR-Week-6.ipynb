{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAHLR Week 6: Gaining early insights\n",
    "\n",
    "Code notebook for TAHLR course at ISAW (Fall 2023) based on Albrecht et al. 2022 (Blueprints) Ch. 1: Gaining Early Insights from Textual Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Installs\n",
    "# ! pip install -U seaborn\n",
    "# ! pip install -U textacy\n",
    "# ! pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get data from remote location\n",
    "\n",
    "# !mkdir -p ../data/blueprints\n",
    "# !curl -LJO https://github.com/blueprints-for-text-analytics-python/blueprints-text/raw/master/data/un-general-debates/un-general-debates-blueprint.csv.gz --output-dir ../data/blueprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "file = \"../data/blueprints/un-general-debates-blueprint.csv.gz\"\n",
    "df = pd.read_csv(file)\n",
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting an overview of the data with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get selection\n",
    "\n",
    "df.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length'] = df['text'].str.len()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['country', 'speaker']].describe(include='O').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check for missing data\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill na for speaker\n",
    "\n",
    "df['speaker'].fillna('unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting value distributions\n",
    "\n",
    "df['length'].plot(kind='box', vert=False, figsize=(10, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length'].plot(kind='hist', bins=30, figsize=(10, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "where = df['country'].isin(['USA', 'FRA', 'GBR', 'CHN', 'RUS'])\n",
    "cp = sns.catplot(data=df[where], x=\"country\", y=\"length\", kind='box', hue='country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp = sns.catplot(data=df[where], x=\"country\", y=\"length\", kind='violin', hue='country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualizing Developments Over Time\n",
    "\n",
    "df.groupby('year').size().plot(title=\"Number of Countries\", figsize=(5, 3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('year').agg({'length': 'mean'}).plot(title=\"Avg. Speech Length\", ylim=(0,30000), figsize=(5, 3));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Building a simple text preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "\n",
    "import regex as re\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r'[\\w-]*\\p{L}[\\w-]*', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Let's defeat SARS-CoV-2 together in 2020!\"\n",
    "tokens = tokenize(text)\n",
    "print(\"|\".join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words\n",
    "\n",
    "import nltk\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop(tokens):\n",
    "    return [t for t in tokens if t.lower() not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = [str.lower, tokenize, remove_stop]\n",
    "\n",
    "def prepare(text, pipeline):\n",
    "    tokens = text\n",
    "    for transform in pipeline:\n",
    "        tokens = transform(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Let's defeat SARS-CoV-2 together in 2020!\"\n",
    "print(prepare(text, pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].apply(str.upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['text'].apply(prepare, pipeline=pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_tokens'] = df['tokens'].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Counting words with a Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tokens = tokenize(\"She likes my cats and my cats like my sofa.\")\n",
    "\n",
    "counter = Counter(tokens)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_tokens = tokenize(\"She likes dogs and cats.\")\n",
    "counter.update(more_tokens)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "\n",
    "df['tokens'].map(counter.update);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(df, column='tokens', preprocess=None, min_freq=2):\n",
    "\n",
    "    # process tokens and update counter\n",
    "    def update(doc):\n",
    "        tokens = doc if preprocess is None else preprocess(doc)\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # create counter and run through all data\n",
    "    counter = Counter()\n",
    "    df[column].map(update)\n",
    "\n",
    "    # transform counter into a DataFrame\n",
    "    freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['freq'])\n",
    "    freq_df = freq_df.query('freq >= @min_freq')\n",
    "    freq_df.index.name = 'token'\n",
    "\n",
    "    return freq_df.sort_values('freq', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df = count_words(df)\n",
    "freq_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_words(df, column='text',\n",
    "                preprocess=lambda text: re.findall(r\"\\w{10,}\", text)).head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Counting a frequency diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = freq_df.head(15).plot(kind='barh', width=0.95)\n",
    "ax.invert_yaxis()\n",
    "ax.set(xlabel='Frequency', ylabel='Token', title='Top Words');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Counting word clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "text = df.query(\"year==2015 and country=='USA'\")['text'].values[0]\n",
    "\n",
    "wc = WordCloud(max_words=100, stopwords=stopwords)\n",
    "wc.generate(text)\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud(word_freq, title=None, max_words=200, stopwords=None):\n",
    "\n",
    "    wc = WordCloud(width=800, height=400,\n",
    "                   background_color= \"black\", colormap=\"Paired\",\n",
    "                   max_font_size=150, max_words=max_words)\n",
    "\n",
    "    # convert DataFrame into dict\n",
    "    if type(word_freq) == pd.Series:\n",
    "        counter = Counter(word_freq.fillna(0).to_dict())\n",
    "    else:\n",
    "        counter = word_freq\n",
    "\n",
    "    # filter stop words in frequency counter\n",
    "    if stopwords is not None:\n",
    "        counter = {token:freq for (token, freq) in counter.items()\n",
    "                              if token not in stopwords}\n",
    "    wc.generate_from_frequencies(counter)\n",
    "\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_2015_df = count_words(df[df['year']==2015])\n",
    "plt.figure()\n",
    "wordcloud(freq_2015_df['freq'], max_words=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud(freq_2015_df['freq'], max_words=100, stopwords=freq_df.head(50).index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Ranking with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(df, column='tokens', preprocess=None, min_df=2):\n",
    "\n",
    "    def update(doc):\n",
    "        tokens = doc if preprocess is None else preprocess(doc)\n",
    "        counter.update(set(tokens))\n",
    "\n",
    "    # count tokens\n",
    "    counter = Counter()\n",
    "    df[column].map(update)\n",
    "\n",
    "    # create DataFrame and compute idf\n",
    "    idf_df = pd.DataFrame.from_dict(counter, orient='index', columns=['df'])\n",
    "    idf_df = idf_df.query('df >= @min_df')\n",
    "    idf_df['idf'] = np.log(len(df)/idf_df['df'])+0.1\n",
    "    idf_df.index.name = 'token'\n",
    "    return idf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_df = compute_idf(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df['tfidf'] = freq_df['freq'] * idf_df['idf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df['tfidf'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_1970 = count_words(df[df['year'] == 1970])\n",
    "freq_2015 = count_words(df[df['year'] == 2015])\n",
    "\n",
    "freq_1970['tfidf'] = freq_1970['freq'] * idf_df['idf']\n",
    "freq_2015['tfidf'] = freq_2015['freq'] * idf_df['idf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud(freq_1970['freq'], title='1970 - TF',\n",
    "          stopwords=['twenty-fifth', 'twenty-five'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud(freq_2015['freq'], title='2015 - TF',\n",
    "          stopwords=['seventieth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud(freq_1970['tfidf'], title='1970 - TF-IDF',\n",
    "          stopwords=['twenty-fifth', 'twenty-five', 'twenty', 'fifth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud(freq_2015['tfidf'], title='2015 - TF-IDF',\n",
    "          stopwords=['seventieth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Finding a keyword-in-context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from textacy.text_utils import KWIC\n",
    "\n",
    "from textacy.extract import keyword_in_context\n",
    "\n",
    "def kwic(doc_series, keyword, window=35, print_samples=5):\n",
    "\n",
    "    def add_kwic(text):\n",
    "        kwic_list.extend(keyword_in_context(text, keyword, ignore_case=True,\n",
    "                              window_width=window))\n",
    "\n",
    "    kwic_list = []\n",
    "    doc_series.map(add_kwic)\n",
    "\n",
    "    if print_samples is None or print_samples==0:\n",
    "        return kwic_list\n",
    "    else:\n",
    "        k = min(print_samples, len(kwic_list))\n",
    "        print(f\"{k} random samples out of {len(kwic_list)} \" + \\\n",
    "              f\"contexts for '{keyword}':\")\n",
    "        for sample in random.sample(list(kwic_list), k):\n",
    "            print(re.sub(r'[\\n\\t]', ' ', sample[0])+'  '+ \\\n",
    "                  sample[1]+'  '+\\\n",
    "                  re.sub(r'[\\n\\t]', ' ', sample[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwic(df[df['year'] == 2015]['text'], 'sdgs', print_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Analyzing n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(tokens, n=2, sep=' '):\n",
    "    return [sep.join(ngram) for ngram in zip(*[tokens[i:] for i in range(n)])]\n",
    "\n",
    "text = \"the visible manifestation of the global climate change\"\n",
    "tokens = tokenize(text)\n",
    "print(\"\\n\".join(ngrams(tokens, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(tokens, n=2, sep=' ', stopwords=set()):\n",
    "    return [sep.join(ngram) for ngram in zip(*[tokens[i:] for i in range(n)])\n",
    "            if len([t for t in ngram if t in stopwords])==0]\n",
    "\n",
    "print(\"Bigrams:\", \"|\".join(ngrams(tokens, 2, stopwords=stopwords)))\n",
    "print(\"Trigrams:\", \"|\".join(ngrams(tokens, 3, stopwords=stopwords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bigrams'] = df['text'].apply(prepare, pipeline=[str.lower, tokenize]) \\\n",
    "                          .apply(ngrams, n=2, stopwords=stopwords)\n",
    "\n",
    "count_words(df, 'bigrams').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate existing IDF DataFrame with bigram IDFs\n",
    "idf_df = pd.concat([idf_df, compute_idf(df, 'bigrams', min_df=10)]);\n",
    "\n",
    "freq_df = count_words(df[df['year'] == 2015], 'bigrams');\n",
    "freq_df['tfidf'] = freq_df['freq'] * idf_df['idf'];\n",
    "wordcloud(freq_df['tfidf'], title='all bigrams', max_words=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Comparing frequencies across time intervals and categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_keywords(tokens, keywords):\n",
    "    tokens = [t for t in tokens if t in keywords]\n",
    "    counter = Counter(tokens)\n",
    "    return [counter.get(k, 0) for k in keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['nuclear', 'terrorism', 'climate', 'freedom']\n",
    "tokens = ['nuclear', 'climate', 'climate', 'freedom', 'climate', 'freedom']\n",
    "\n",
    "print(count_keywords(tokens, keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_keywords_by(df, by, keywords, column='tokens'):\n",
    "\n",
    "    freq_matrix = df[column].apply(count_keywords, keywords=keywords)\n",
    "    freq_df = pd.DataFrame.from_records(freq_matrix, columns=keywords)\n",
    "    freq_df[by] = df[by] # copy the grouping column(s)\n",
    "\n",
    "    return freq_df.groupby(by=by).sum().sort_values(by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df = count_keywords_by(df, by='year', keywords=keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df.plot(kind='line', figsize=(10,3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['terrorism', 'terrorist', 'nuclear', 'war', 'oil',\n",
    "            'syria', 'syrian', 'refugees', 'migration', 'peacekeeping',\n",
    "            'humanitarian', 'climate', 'change', 'sustainable', 'sdgs']\n",
    "\n",
    "freq_df = count_keywords_by(df, by='year', keywords=keywords)\n",
    "\n",
    "# compute relative frequencies based on total number of tokens per year\n",
    "freq_df = freq_df.div(df.groupby('year')['num_tokens'].sum(), axis=0)\n",
    "# apply square root as sublinear filter for better contrast\n",
    "freq_df = freq_df.apply(np.sqrt)\n",
    "\n",
    "plt.figure(figsize = (10,5))\n",
    "ax = sns.heatmap(data=freq_df.T,\n",
    "            xticklabels=True, yticklabels=True, cbar=False, cmap=\"Reds\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Albrecht et al. \"things to consider\" for keyword analysis\n",
    "# - Prefer relative frequencies for any kind of comparison.\n",
    "# - Be careful with the interpretation of frequency diagrams based on keyword lists.\n",
    "# - Use sublinear scaling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tahlr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
